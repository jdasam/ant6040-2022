{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdasam/ant6040-2022/blob/main/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxm1A6x9Pbx6"
      },
      "source": [
        "# Assignment 1. Maps of Images\n",
        "- The goal of this assignment is to understand the embeddings of deep-learning-based image classification model\n",
        "  - By visualizing the image embedding into 2-dim space, you can see how the model clusters the images\n",
        "\n",
        "- If you run this notebook on Colab, it is highly recommended to use GPU, to calculate the embeddings faster\n",
        "\n",
        ".\n",
        "- Evaluation Criteria\n",
        "  - The assignment expects you to achieve either of these two goals:\n",
        "    - a. Discover and explain what kind of visual characteristics does the model uses as important features\n",
        "      - Your explanation can be just your own hypothesis. But please try to find an example or evidence to support your hypothesis.\n",
        "      - Note that the pre-trained model was trained with ImageNet 1K\n",
        "        - [List of Labels in ImageNet 1k](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)\n",
        "        - [Sample Images](https://github.com/EliSchwartz/imagenet-sample-images)\n",
        "    - b. Find interesting examples on how the images distribute in the embedding space\n",
        "      - Explain why those examples were interesting to you.\n",
        "\n",
        ".\n",
        "\n",
        "- Select your own images to complete this assignment\n",
        "  - Selecting an interesting image set would be the important part of the assignment\n",
        "  - You can download existing dataset or crawl images from web using your own keywords\n",
        "  - Or you can manually import dataset of your selection\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h98nlmgWPbx7"
      },
      "source": [
        "## Preparation: Install and import libraries\n",
        "- Running the cells below will automatically install and import libraries you need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu77owApPbx7"
      },
      "outputs": [],
      "source": [
        "!pip install umap-learn # install umap\n",
        "# install jmd_imagescraper\n",
        "!pip install -q jmd_imagescraper\n",
        "!git clone https://github.com/Quasimondo/RasterFairy.git\n",
        "%cd RasterFairy\n",
        "!pip install .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Na_bnEEOPbx8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "You don't have to change codes in this cell\n",
        "'''\n",
        "\n",
        "from jmd_imagescraper.core import duckduckgo_search\n",
        "from pathlib import Path\n",
        "import random\n",
        "import shutil\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "import rasterfairy\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "def get_image_files(path_dir):\n",
        "  if isinstance(path_dir, str):\n",
        "    path_dir = Path(path_dir)  \n",
        "  return list(path_dir.rglob('*.jpg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W3IymTLPbx8"
      },
      "source": [
        "## 0. Collect your image\n",
        "- There are two options:\n",
        "  - Option A: Use Artbench (a dataset used during the class)\n",
        "    - Or you might download other image dataset from web\n",
        "  - Option B: Download images using web crawler. You can select search keywords of your own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zv5OM-Pbx8"
      },
      "source": [
        "### Option A. Use ArtBench (or other dataset)\n",
        "- You can use ArtBench dataset, which consists 60,000 paintings of of 10 different artistic styles.\n",
        "- If you have want to use other datasets, you can also use them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-yiPtTFPbx8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Warnings: This cell will download a ArtBench dataset images.\n",
        "'''\n",
        "!wget https://artbench.eecs.berkeley.edu/files/artbench-10-imagefolder-split.tar\n",
        "!tar -xvf artbench-10-imagefolder-split.tar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLSkXiU4Pbx8"
      },
      "source": [
        "### Option b. Crawl Images by Keywords\n",
        "- The code below will automatically downloads images that are searched by `image_keywords` in subdirectory of `images/`\n",
        "  - You can write down your own keywords in the `image_keywords` list.\n",
        "  - You can set number of images per each keyword with `NUM_IMG`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFI6m3OdPbx8"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "TODO: Write your own image_types list to search for different types of images\n",
        "'''\n",
        "image_keywords = [\"French Art\", \"German Art\", \"British Art\", \"Russian Art\"]\n",
        "NUM_IMG = 50\n",
        "\n",
        "assert all(isinstance(typ, str) for typ in image_keywords), \"Every element of image_types has to be string\"\n",
        "\n",
        "'''\n",
        "Warning: Running this cell will delete the content of the folder \"images/\", and download new images\n",
        "'''\n",
        "!rm -rf images # Delete the previous images automatically\n",
        "img_dir = Path('images')\n",
        "# train_dir = img_dir / 'train'\n",
        "# valid_dir = img_dir / 'valid'\n",
        "for typ in image_keywords:\n",
        "  duckduckgo_search(img_dir, typ, typ, max_results=NUM_IMG)\n",
        "  typ_dir = img_dir / typ\n",
        "\n",
        "\n",
        "# Code below will split the images into train and validation set\n",
        "random.seed(0)\n",
        "valid_indices = random.sample(range(NUM_IMG), NUM_IMG//5)\n",
        "for typ in image_keywords:\n",
        "  typ_dir = img_dir / typ\n",
        "  if not typ_dir.exists():\n",
        "    continue\n",
        "  train_dir = img_dir / 'train' / typ\n",
        "  test_dir = img_dir / 'test' / typ\n",
        "  train_dir.mkdir(parents=True, exist_ok=True)\n",
        "  test_dir.mkdir(parents=True, exist_ok=True)\n",
        "  img_files = get_image_files(typ_dir)\n",
        "  valid_imgs = [img_files[i] for i in valid_indices]\n",
        "  for fn in valid_imgs:\n",
        "    shutil.move(fn, test_dir/fn.name)\n",
        "  img_files = get_image_files(typ_dir)\n",
        "  for fn in img_files:\n",
        "    shutil.move(fn, train_dir/fn.name)\n",
        "  assert len(get_image_files(typ_dir)) == 0, f\"There are still remaining files in {typ_dir}\"\n",
        "  os.rmdir(typ_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "312BxDTkPbx9"
      },
      "source": [
        "### 0.2. Make Dataset\n",
        "- The pre-defined `ImageSet` class will list-up the path for every images with `jpg` and `png` (in default)\n",
        "  - The list of image paths is saved as `image_fns`\n",
        "  - The label of each image is automatically set by it's parent directory's name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewHYcugwPbx9"
      },
      "outputs": [],
      "source": [
        "class ImageSet:\n",
        "  def __init__(self, path_dir, file_types=['jpg', 'png'], transform=None):\n",
        "    self.path = Path(path_dir)\n",
        "    self.image_fns = sorted(item for y in [list(self.path.rglob(f'*.{x}')) for x in file_types] for item in y) \n",
        "    self.classes = sorted(list(set([x.parent.name for x in self.image_fns])))\n",
        "    self.cls2idx = {k: i for i, k in enumerate(self.classes)}\n",
        "    self.transform = transforms.Compose([\n",
        "                        transforms.Resize(256),\n",
        "                        transforms.CenterCrop(224),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.image_fns)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    img_path = self.image_fns[idx]\n",
        "    img = self.transform(Image.open(img_path))\n",
        "    # img = Image.open(img_path)\n",
        "    cls = img_path.parent.name\n",
        "    return img, self.cls2idx[cls]\n",
        "\n",
        "\n",
        "'''\n",
        "TODO: Select a directory path for your dataset\n",
        "'''\n",
        "\n",
        "# Option A: use ArtBench dataset (test split only in default)\n",
        "# dataset = ImageSet('data/artbench-10-imagefolder-split/test')\n",
        "\n",
        "# Option B: use crawled dataset\n",
        "# You can use both train and test set, by selecting path as 'images/'\n",
        "dataset = ImageSet('images/')\n",
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPTX2j33Pbx9"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Show Batch Examples\n",
        "\n",
        "Because we have used shuffle=True in the ImageSet class, the images will be shown in a random order.\n",
        "'''\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "batch = next(iter(dataloader))\n",
        "\n",
        "tensor2pil = transforms.Compose([\n",
        "    transforms.Normalize(mean=[0, 0, 0], std=[4.3668, 4.4643, 4.4444]),\n",
        "    transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1, 1, 1]),\n",
        "    transforms.ToPILImage()\n",
        "])\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "for i in range(16):\n",
        "  plt.subplot(4, 4, i+1)\n",
        "  plt.imshow(tensor2pil(batch[0][i]))\n",
        "  plt.axis('off')\n",
        "  plt.title(dataset.classes[batch[1][i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPmiZM0QPbx9"
      },
      "source": [
        "## 1. Load model and make embeddings of the images\n",
        "- To make running faster, `resnet18` is selected as a default model\n",
        "  - You can select other models if you want.\n",
        "- To get the final embedding of the model, which is an input for the resnet18.fc, we register a forward hook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2TFWFvGPbx9"
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "embedding = None\n",
        "def hook(module, input, output):\n",
        "  global embedding\n",
        "  embedding = input\n",
        "model.fc.register_forward_hook(hook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l2JXXdrPbx9"
      },
      "outputs": [],
      "source": [
        "class EmbeddingVisualizer:\n",
        "  def __init__(self, dataset, model, emb_method='umap', device=None, batch_size=16):\n",
        "    self.dataset = dataset\n",
        "    self.model = model\n",
        "    if device is None:\n",
        "      self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(self.device)\n",
        "    self.model.eval()\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    print(\"Calculating Embeddings...\")\n",
        "    self.embeddings = self.get_embedding()\n",
        "    self.red_embs, self.reducer = self.get_reduced_embedding(emb_method)\n",
        "    \n",
        "    self.center_crop = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor()])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "  \n",
        "  def get_embedding(self):\n",
        "    global embedding\n",
        "    dataloader = torch.utils.data.DataLoader(self.dataset, self.batch_size, shuffle=False)\n",
        "    embeddings = []\n",
        "    \n",
        "    embedding = None\n",
        "    with torch.inference_mode():\n",
        "      for batch in tqdm(dataloader):\n",
        "        img, label = batch\n",
        "        img = img.to(self.device)\n",
        "        model(img)\n",
        "        embeddings.append(embedding[0])\n",
        "        embedding = None\n",
        "\n",
        "    embeddings = torch.cat(embeddings, dim=0).cpu()\n",
        "    return embeddings\n",
        "\n",
        "  def get_reduced_embedding(self, method='umap', n_components=2):\n",
        "    if method == 'umap':\n",
        "      reducer = umap.UMAP(n_components=n_components)\n",
        "      embedding = reducer.fit_transform(self.embeddings.cpu().numpy())\n",
        "    elif method == 'tsne':\n",
        "      reducer = TSNE(n_components=n_components)\n",
        "    elif method == 'pca':\n",
        "      reducer = PCA(n_components=n_components)\n",
        "    else:\n",
        "      raise NotImplementedError(f\"{method} is not implemented\")\n",
        "\n",
        "    return reducer.fit_transform(self.embeddings.cpu().numpy()), reducer\n",
        "\n",
        "  def get_image(self, path, zoom=0.2):\n",
        "    img = Image.open(path)\n",
        "    return OffsetImage(self.center_crop(img).permute(1,2,0).numpy(), zoom=zoom)\n",
        "\n",
        "  def plot_images_on_embedding_path(self, embeddings, input_a, input_b, k=10, add_paths=None):\n",
        "    if isinstance(input_a, int) and isinstance(input_b, int):\n",
        "      emb_a = embeddings[input_a]\n",
        "      emb_b = embeddings[input_b]\n",
        "    elif isinstance(input_a, torch.Tensor) and isinstance(input_b, torch.Tensor):\n",
        "      emb_a = input_a\n",
        "      emb_b = input_b\n",
        "    else:\n",
        "      raise ValueError(\"input_a and input_b must be either int or torch.Tensor\")\n",
        "\n",
        "    mid_embs = []\n",
        "    for i in range(0, k+1):\n",
        "      t = i / k\n",
        "      emb = emb_a * (1-t) + emb_b * t\n",
        "      mid_embs.append(emb)\n",
        "    mid_embs = torch.stack(mid_embs, dim=0)\n",
        "    dists = torch.cdist(mid_embs, embeddings)\n",
        "    nearest_idx = dists.argsort(dim=1)[:, 0]\n",
        "    nearest_paths = [self.dataset.image_fns[idx] for idx in nearest_idx]\n",
        "    if isinstance(add_paths, list) or isinstance(add_paths, tuple):\n",
        "      nearest_paths = add_paths[0:1] + nearest_paths + add_paths[1:]\n",
        "    images = [Image.open(path) for path in nearest_paths]\n",
        "\n",
        "    fig, ax = plt.subplots(1, len(nearest_paths), figsize=(30, 5))\n",
        "    for i, image in enumerate(images):\n",
        "      plt.subplot(1,  len(nearest_paths), i+1)\n",
        "      plt.imshow(image)\n",
        "      plt.axis('off')\n",
        "\n",
        "\n",
        "  def plot_embeddings_with_image(self, embeddings, num, figsize=(20, 20), zoom=0.2, rand_seed=0):\n",
        "    if num < len(embeddings):\n",
        "      np.random.seed(rand_seed)\n",
        "      idxs = np.random.choice(len(embeddings), num, replace=False)\n",
        "    else:\n",
        "      idxs = np.arange(len(embeddings))\n",
        "    paths = [str(self.dataset.image_fns[i]) for i in idxs]\n",
        "\n",
        "    x = embeddings[idxs, 0]\n",
        "    y = embeddings[idxs, 1]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.scatter(x, y) \n",
        "\n",
        "    for x0, y0, path in zip(x, y,paths):\n",
        "        ab = AnnotationBbox(self.get_image(path, zoom), (x0, y0), frameon=False)\n",
        "        ax.add_artist(ab)\n",
        "\n",
        "  def plot_rasterfairy_embeddings(self, embeddings, num, figsize=20, zoom=0.2, rand_seed=0):\n",
        "    if num < len(embeddings):\n",
        "      np.random.seed(rand_seed)\n",
        "      idxs = np.random.choice(len(embeddings), num, replace=False)\n",
        "    else:\n",
        "      idxs = np.arange(len(embeddings))\n",
        "    paths = [str(self.dataset.image_fns[i]) for i in idxs]\n",
        "\n",
        "    grid_xy, shape = rasterfairy.transformPointCloud2D(embeddings[idxs])\n",
        "    new_figsize = (figsize / 20 * shape[0], figsize / 20 * shape[1] )\n",
        "    fig, ax = plt.subplots(figsize=new_figsize)\n",
        "    x, y = grid_xy[:, 0], grid_xy[:, 1]\n",
        "    ax.scatter(x, y) \n",
        "\n",
        "    for x0, y0, path in zip(x, y,paths):\n",
        "        ab = AnnotationBbox(self.get_image(path, zoom), (x0, y0), frameon=False)\n",
        "        ax.add_artist(ab)\n",
        "\n",
        "  def get_embedding_from_image_path(self, path):\n",
        "    img = Image.open(path)\n",
        "    img = self.center_crop(img)\n",
        "    img = img.unsqueeze(0)\n",
        "    img = img.to(self.device)\n",
        "    with torch.inference_mode():\n",
        "      model(img)\n",
        "    return embedding[0].cpu()\n",
        "\n",
        "  def plot_most_similar_images(self, path, num=10):\n",
        "    emb = self.get_embedding_from_image_path(path)\n",
        "    dists = torch.cdist(emb, self.embeddings)\n",
        "    nearest_idx = dists.argsort(dim=1)[:, 0:num]\n",
        "    print(nearest_idx.shape)\n",
        "    nearest_paths = [self.dataset.image_fns[idx] for idx in nearest_idx[0]]\n",
        "    images = [Image.open(path) for path in nearest_paths]\n",
        "    fig, ax = plt.subplots(1, len(images), figsize=(30, 5))\n",
        "    for i, image in enumerate(images):\n",
        "      plt.subplot(1,  len(images), i+1)\n",
        "      plt.imshow(image)\n",
        "      plt.axis('off')\n",
        "\n",
        "\n",
        "  def plot_path_between_two_images(self, path_a, path_b, pca_n_comp=2, k=10):\n",
        "    emb_a = self.get_embedding_from_image_path(path_a)\n",
        "    emb_b = self.get_embedding_from_image_path(path_b)\n",
        "    pca_embs, reducer = self.get_reduced_embedding('pca', n_components=4)\n",
        "    pca_embs = torch.tensor(pca_embs, dtype=torch.float32)\n",
        "    emb_a = torch.tensor(reducer.transform(emb_a), dtype=torch.float32)\n",
        "    emb_b = torch.tensor(reducer.transform(emb_b), dtype=torch.float32)\n",
        "    self.plot_images_on_embedding_path(pca_embs, emb_a[0], emb_b[0], k=k, add_paths=[path_a, path_b])\n",
        "\n",
        "\n",
        "\n",
        "visualizer = EmbeddingVisualizer(dataset, model, 'umap')\n",
        "visualizer.embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY5I6aIMPbx-"
      },
      "source": [
        "### 1.1 Plot Scatter of Images\n",
        "- In default, it will visulzied 2D-reduced embeddings using UMAP\n",
        "  - You can try `tsne` or `pca` using codes as below:\n",
        "    - `visualizer.plot_embeddings_with_image(visualizer.get_reduced_embedding(method='tsne')[0], num=200, zoom=0.4, rand_seed=1)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILCfO3mHPbx-"
      },
      "outputs": [],
      "source": [
        "visualizer.plot_embeddings_with_image(visualizer.red_embs, num=200, zoom=0.4, rand_seed=10)\n",
        "\n",
        "# Other options: 'pca', 'tsne', 'umap'\n",
        "# visualizer.plot_embeddings_with_image(visualizer.get_reduced_embedding(method='tsne')[0], num=200, zoom=0.4, rand_seed=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhN-jYl7Pbx-"
      },
      "outputs": [],
      "source": [
        "# figsize for this function takes only scalar value. \n",
        "# figsize 20 and zoom 0.2 is default settings\n",
        "visualizer.plot_rasterfairy_embeddings(visualizer.red_embs, num=200, figsize=20, zoom=0.2, rand_seed=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxH4FNMQPbx-"
      },
      "source": [
        "# 2. Plot paths between images\n",
        "- Depends on the density of your dataset, you can select different `n_components` of PCA\n",
        "  - If you use too high `n_components` compared to number of images in the dataset, there could be no image embeddings at all between the two given image embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPABWLMBPbx-"
      },
      "outputs": [],
      "source": [
        "pca_embedding = visualizer.get_reduced_embedding('pca', n_components=2)[0]\n",
        "visualizer.plot_images_on_embedding_path(torch.tensor(pca_embedding), input_a=0, input_b=1, k=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use your own image\n",
        "- You can load your own image to get embedding and plot similar images or images between two images\n",
        "  - If you are using Colab, you can upload it via file-explorer, which is the panel in the left side"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6ausm6CPbx_"
      },
      "outputs": [],
      "source": [
        "# TODO: you can use your own image path\n",
        "your_image_path = ''\n",
        "visualizer.plot_most_similar_images(your_image_path, num=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEDZ6OhoPbx_"
      },
      "outputs": [],
      "source": [
        "your_image_path_a = 'write down your image path here'\n",
        "your_image_path_b = 'write down your image path here'\n",
        "\n",
        "visualizer.plot_path_between_two_images(your_image_path_a, your_image_path_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBlkJyzRPbx_"
      },
      "source": [
        "## 3 (Optional). Fine tune Model and Visualize Again\n",
        "- Different model sees different things from the same image.\n",
        "  - If you fine tune the model to classify your own dataset (instead of ImageNet), the latent space will be changed\n",
        "- It is not mandatory to split train/validation/test in strict way to observe latent space\n",
        "  - But if you want to check how well is the fine tune going on, it is better to use train/test split, at least."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXYLHpNCPbx_",
        "outputId": "facda03d-0061-429e-8f67-6b556f4d60eb"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TODO: Fill in your training and test set paths to the trainset and testset\n",
        "'''\n",
        "\n",
        "# trainset_path = 'artbench-10-imagefolder-split/train'\n",
        "# testset_path = 'artbench-10-imagefolder-split/test'\n",
        "trainset_path = 'images/train/'\n",
        "testset_path = 'images/test/'\n",
        "\n",
        "\n",
        "trainset = ImageSet(trainset_path)\n",
        "testset = ImageSet(testset_path)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=False)\n",
        "\n",
        "new_model = torchvision.models.resnet18(pretrained=True)\n",
        "new_model.fc = torch.nn.Linear(new_model.fc.in_features, len(dataset.classes))\n",
        "\n",
        "new_model.fc.register_forward_hook(hook)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "optimizer = torch.optim.Adam(new_model.parameters(), lr=0.0003)\n",
        "new_model = new_model.to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBNQa1DFPbx_"
      },
      "outputs": [],
      "source": [
        "# fine tune your model\n",
        "\n",
        "def get_accuracy(model, data_loader, device):\n",
        "  model.to(device)\n",
        "  accuracy = 0\n",
        "  for batch in data_loader:\n",
        "    images, labels = batch\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    with torch.inference_mode():\n",
        "      out = model(images)\n",
        "    accuracy += (out.argmax(dim=1) == labels).sum().item()\n",
        "  return accuracy / len(data_loader.dataset)\n",
        "\n",
        "\n",
        "n_epochs = 10\n",
        "loss_record = []\n",
        "accuracies = []\n",
        "\n",
        "new_model.to(device)\n",
        "for i in range(n_epochs):\n",
        "  new_model.train()\n",
        "  for batch in train_loader:\n",
        "    images, labels = batch\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    out = new_model(images)\n",
        "    loss = criterion(out, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_record.append(loss.item())\n",
        "  new_model.eval()\n",
        "  accuracy = get_accuracy(new_model, test_loader, device)\n",
        "  accuracies.append(accuracy)\n",
        "  print(f'Epoch {i+1} accuracy: {accuracy}')\n",
        "\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(loss_record)\n",
        "plt.title('Training Loss')\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(accuracies)\n",
        "plt.title('Test Accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyTeWxzFPbx_"
      },
      "outputs": [],
      "source": [
        "new_visualizer = EmbeddingVisualizer(dataset, new_model, 'umap')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN1A4yr4Pbx_"
      },
      "outputs": [],
      "source": [
        "new_visualizer.plot_embeddings_with_image(new_visualizer.red_embs, 200, zoom=0.4, rand_seed=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7s7tCPzPbx_"
      },
      "outputs": [],
      "source": [
        "new_visualizer.plot_rasterfairy_embeddings(new_visualizer.red_embs, 500, figsize=20, zoom=0.2, rand_seed=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
